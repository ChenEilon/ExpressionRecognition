{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from imutils import face_utils\n",
    "#import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import os\n",
    "#import time\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import glob\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn import svm\n",
    "#import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "REF_POINTS = [1, 4, 14, 17, 18, 20, 22, 23, 25, 27, 28, 31, 32, 36, 37, 38, 40, 42, 43, 45, 46, 47, 49, 51, 52, 53, 61, 63, 65, 67]\n",
    "EMOTIONS = [\"neutral\",  \"happy\", \"sadness\", \"surprise\",  \"fear\", \"disgust\", \"anger\"]\n",
    "wanted_landmarks = [i-1 for i in REF_POINTS]\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "##############                   Math and transformations                  ############\n",
    "#######################################################################################\n",
    "def squared_distance(x,y):\n",
    "    return (x[0]-y[0])**2+(x[1]-y[1])**2\n",
    "\n",
    "def rect_to_bb(rect):\n",
    "    \"\"\" take a bounding predicted by dlib and convert it\n",
    "     to the format (x, y, w, h) as we would normally do\n",
    "     with OpenCV\n",
    "     *from web tutorial*\"\"\"\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "    return (x, y, w, h) # return a tuple of (x, y, w, h)\n",
    "\n",
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    \"\"\"\n",
    "    *from web tutorial*\n",
    "    \"\"\"\n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coords = np.zeros((68, 2), dtype=dtype)\n",
    "    # loop over the 68 facial landmarks and convert them\n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coords\n",
    "\n",
    "def nparray_to_pandas_images(faces_68_landmarks):\n",
    "    \"\"\"\n",
    "    input - nparray of numpy array (list of images numpy array, which contains 68 cords(tuple))\n",
    "    output - pandas dataframe of data\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame.from_records(faces_68_landmarks)\n",
    "    return df\n",
    "    \n",
    "def dataset_from_affectnet(trainingCsvPath):\n",
    "    \"\"\"\n",
    "    problem with affectnet landmarks...\n",
    "    \"\"\"\n",
    "    data_df = pd.read_csv(trainingCsvPath)\n",
    "    df_filtered = data_df.query('expression<8')\n",
    "    landmarks = (df_filtered[['facial_landmarks']].values).flatten()\n",
    "    labels = df_filtered[['expression']].values\n",
    "    facial_landmarks_data = [np.reshape(i.split(\";\"),(68,2)) for i in landmarks]\n",
    "    return (facial_landmarks_data, labels.flatten())\n",
    "\n",
    "    \n",
    "#######################################################################################\n",
    "##############                   Point Methods                             ############\n",
    "#######################################################################################\n",
    "\n",
    "def dot_matrix(point_arr):\n",
    "    \"\"\"\n",
    "    input - nparray of (x, y) points\n",
    "    output - an nxn matrix M where M[i, j] is the dot product of (xi, yi) and (xj, yj)\n",
    "    \"\"\"\n",
    "    dot_m = np.ndarray(shape=(len(point_arr), len(point_arr)), dtype=int)\n",
    "    for i in range(len(point_arr)):\n",
    "        for j in range(i+1):\n",
    "            dot_m[i, j] = np.dot(point_arr[i], point_arr[j])\n",
    "            dot_m[j, i] = dot_m[i, j]\n",
    "    return dot_m\n",
    "\n",
    "def dist_matrix(dot_m):\n",
    "    \"\"\"\n",
    "    input - a dot matrix (output of dot_matrix method)\n",
    "    output - an nxn matrix M where M[i, j] is the distance between (xi, yi) and (xj, yj)\n",
    "    \"\"\"\n",
    "    dist_m = np.ndarray(shape=dot_m.shape, dtype=float)\n",
    "    for i in range(dist_m.shape[0]):\n",
    "        dist_m[i, i] = 0\n",
    "        for j in range(i):\n",
    "            dist_m[i, j] = np.sqrt(dot_m[i, i] - 2*dot_m[i, j] + dot_m[j, j])\n",
    "            dist_m[j, i] = dist_m[i, j]\n",
    "    return dist_m\n",
    "\n",
    "def dist_array(dist_m):\n",
    "    \"\"\"\n",
    "    input - a distance matrix (output of dist_matrix method)\n",
    "    output - an array of all of the distances, w/o duplicates\n",
    "    \"\"\"\n",
    "    dists = []\n",
    "    for i in range(dist_m.shape[0]):\n",
    "        for j in range(i):\n",
    "            dists.append(dist_m[i, j])\n",
    "    return np.array(dists)\n",
    "\n",
    "def angle_array(dot_m, dist_m):\n",
    "    \"\"\"\n",
    "    input - a dot matrix (output of dot_matrix method), a distance matrix (output of dist_matrix method)\n",
    "    output - an array of all of the angles, w/o duplicates\n",
    "    \"\"\"\n",
    "    angles = []\n",
    "    for i in range(dot_m.shape[0]):\n",
    "        for j in range(i):\n",
    "            for k in range(j):\n",
    "                #TODO change solution to devision by 0\n",
    "                if not (dist_m[i, j] * dist_m[j, k] * dist_m[i, k]):\n",
    "                    angles.append(-1)\n",
    "                    angles.append(-1)\n",
    "                    # angles.append(-1)\n",
    "                else:\n",
    "                    angles.append(np.arccos(round(\n",
    "                        (dot_m[i, k] - dot_m[i, j] - dot_m[j, k] + dot_m[j, j]) / (dist_m[i, j] * dist_m[j, k]),\n",
    "                        15)))\n",
    "                    angles.append(np.arccos(round(\n",
    "                        (dot_m[i, j] - dot_m[i, k] - dot_m[k, j] + dot_m[k, k]) / (dist_m[i, k] * dist_m[k, j]),\n",
    "                        15)))\n",
    "                    # angles.append(np.pi - angles[-1] - angles[-2])\n",
    "    return np.array(angles)\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "##############            Detecting face and face landmarks                ############\n",
    "#######################################################################################\n",
    "    \n",
    "def image_to_landmarks(image_path, detector, predictor):\n",
    "    \"\"\"assuming an image\"\"\"\n",
    "    # load the input image, resize it, and convert it to grayscale\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return []\n",
    "    image = imutils.resize(image, width=350)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # detect faces in the grayscale image\n",
    "    rects = detector(gray, 1)\n",
    "    # determine the facial landmarks for the face region, then\n",
    "    # convert the facial landmark (x, y)-coordinates to a NumPy array\n",
    "    if len(rects)==0:\n",
    "        return []\n",
    "    shape = predictor(gray, rects[0])\n",
    "    shape = face_utils.shape_to_np(shape)\n",
    "    #faces_68_landmarks.append(shape)\n",
    "    return shape\n",
    "\n",
    "def sort_sample_affectnet(inputFolder, csvPathAffectnet, start=0, count=10000):\n",
    "    \"\"\"\n",
    "    csv: 'image_name', 'expression', '68_landmarks'\n",
    "    \"\"\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    data_df = pd.read_csv(csvPathAffectnet)\n",
    "    landmarks = []\n",
    "    # Gal's lines, do not touch!\n",
    "    # folders = glob.glob(inputFolder + \"\\\\*\") #Returns a list of all folders with participant numbers\n",
    "    # for folder in folders:\n",
    "    #     files = glob.glob(folder + \"\\\\*\")\n",
    "    #     for f in files:\n",
    "    #         shape = image_to_landmarks(f, detector, predictor)\n",
    "    #         shape = list(np.array(shape).flatten())\n",
    "    #         img_name = [(f.split(\"\\\\\"))[-1]]\n",
    "    #         landmarks.append(img_name + shape)\n",
    "    for i in range(start, start+count):\n",
    "        f = \"{0}\\\\{1}\\\\{2}\".format(inputFolder, data_df.loc[i, \"subDirectory\"], data_df.loc[i, \"filePath\"])\n",
    "        shape = image_to_landmarks(f, detector, predictor)\n",
    "        shape = list(np.array(shape).flatten())\n",
    "        img_name = [(f.split(\"\\\\\"))[-1]]\n",
    "        landmarks.append(img_name + shape)\n",
    "    cols = [\"filePath\"] + [\"x_{0:d}\".format(i//2) if i%2==0 else \"y_{0:d}\".format(i//2) for i in range(2, 69*2)]\n",
    "    landmarks_df = pd.DataFrame(landmarks, columns=cols, index=np.arange(start, start+count))\n",
    "    if start == 0:\n",
    "        data_df = data_df.merge(landmarks_df, on=\"filePath\", how=\"left\")\n",
    "        data_df.to_csv('affectnet_landmarks.csv', index=False)\n",
    "    else:\n",
    "        data_df.update(landmarks_df)\n",
    "        data_df.to_csv(csvPathAffectnet, index=False)\n",
    "\n",
    "def add_expression_dummies(features_df):\n",
    "    for i in range(len(EMOTIONS)):\n",
    "        features_df[\"is_{0:s}\".format(EMOTIONS[i])] = (features_df[\"expression\"] == i)\n",
    "    features_df.drop(\"expression\", axis=1, inplace=True)\n",
    "\n",
    "def csv_to_features(csvDirPath, maxRows=2000, filePrefix=\"affectnet_landmarks\"):\n",
    "    \"\"\"\n",
    "    in - csv from sort_sample_affectnet\n",
    "    out - features dataframe\n",
    "    \"\"\"\n",
    "    col_names = []\n",
    "    for i in REF_POINTS:\n",
    "        col_names.append(\"x_{0:d}\".format(i))\n",
    "        col_names.append(\"y_{0:d}\".format(i))\n",
    "    filenames = [entry.name for entry in os.scandir(csvDirPath) if entry.name.endswith(\".csv\") and entry.name.startswith(filePrefix)]\n",
    "    for f in filenames:\n",
    "        print(\"Processing {0}\".format(f))\n",
    "        data_df = pd.read_csv(os.path.join(csvDirPath, f))\n",
    "        df_filtered = data_df.query('expression<=7').dropna().iloc[:maxRows, :] #filter out non-faces\n",
    "        #ndarray of wanted landmarks (row per image)\n",
    "        images_df = df_filtered[col_names]\n",
    "        images_df = np.reshape(images_df.values.astype(int), (len(images_df), len(REF_POINTS), 2))\n",
    "        #extract features\n",
    "        features_df = extract_features_forall(images_df)\n",
    "        features_df[\"expression\"] = df_filtered[\"expression\"].values\n",
    "        add_expression_dummies(features_df)\n",
    "        features_df.to_csv(os.path.join(csvDirPath, \"features_{0}\".format(f)))\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "##############            Extract features and reducing dimensions         ############\n",
    "#######################################################################################\n",
    "\n",
    "def reduce_correlated_cols(df, threshold=0.95):\n",
    "    \"\"\"\n",
    "    input - df &threshold (if 1>|correlation|>threshold then dimension is reduced\n",
    "    output - reduced df\n",
    "    \"\"\"\n",
    "    corr = df.corr()\n",
    "    corr = corr * np.fromfunction(lambda i, j: i > j, corr.shape)\n",
    "    corr_cols = (corr > threshold).sum(axis=1)\n",
    "    corr_cols = corr_cols[corr_cols > 0].axes[0].tolist()\n",
    "    ret = df.drop(corr_cols, axis=1)\n",
    "    return ret\n",
    "\n",
    "def dimension_reduction_pca(df, components = 100):\n",
    "    \"\"\"\n",
    "    input - dataframe of features & wanted dimension of features\n",
    "    output - trained PCA\n",
    "    uses PCA from skylearn\n",
    "    \"\"\"\n",
    "    #Standardize the Data\n",
    "    features = list(df.columns.values)\n",
    "    # Separating out the features\n",
    "    x = df.loc[:, features].values\n",
    "    # Standardizing the features\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    #dim reduction\n",
    "    pca = PCA(components)\n",
    "    pca.fit_transform(x)\n",
    "    return pca\n",
    "    \n",
    "\n",
    "#######################################################################################\n",
    "##############            Machine learning algorithms                      ############\n",
    "#######################################################################################\n",
    "\n",
    "# logistic regression\n",
    "def log_reg_classifier(imgs_features, imgs_lbls, c=1):\n",
    "    \"\"\"\n",
    "    input - list of featurs list\n",
    "    output - logistic regression classifier\n",
    "    \"\"\"\n",
    "    clf = LogisticRegression(C = c, penalty = 'l2') #TODO check best C\n",
    "    return clf.fit(imgs_features, imgs_lbls)\n",
    "\n",
    "# SVM\n",
    "def svm_classifier(imgs_features, imgs_lbls, c=1):\n",
    "    \"\"\"\n",
    "    input - list of featurs list\n",
    "    output - svm classifier\n",
    "    \"\"\"\n",
    "    # Create a classifier: a support vector classifier\n",
    "    svm_classifier = svm.SVC(C = c) #TODO check best C\n",
    "    # training\n",
    "    return svm_classifier.fit(imgs_features, imgs_lbls)\n",
    "    \n",
    "# KNN\n",
    "def knn_classifier(imgs_features, imgs_lbls, k=1):\n",
    "    \"\"\"\n",
    "    input - list of featurs list\n",
    "    output - knn classifier\n",
    "    \"\"\"\n",
    "    knn = KNeighborsClassifier(n_neighbors = k) #TODO check best k\n",
    "    return knn.fit(imgs_features, imgs_lbls) \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dlib_facial_points(inputFolder):\n",
    "    \"\"\"\n",
    "    input - images folder name\n",
    "    output - ndarray of images facial landmarks \n",
    "    \"\"\"\n",
    "    wanted_landmarks = [i-1 for i in REF_POINTS]\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    files = glob.glob(\"%s\\\\*\"%inputFolder) #Get list of all images in inputFolder\n",
    "    faces_landmarks = []\n",
    "    for f in files:\n",
    "        if f.lower().endswith(\".png\") or f.lower().endswith(\".jpg\") or f.lower().endswith(\".jpeg\"): \n",
    "            shape = np.array(image_to_landmarks(f, detector, predictor))\n",
    "            if (shape.size!=0):\n",
    "                shape = shape[wanted_landmarks]\n",
    "            faces_landmarks.append(shape)\n",
    "    return np.array(faces_landmarks),files\n",
    "\n",
    "def extract_features(image, f):\n",
    "    \"\"\"\n",
    "    input - nparray of facial landmarks (point (x,y))\n",
    "    output - nparray of features per image\n",
    "    \"\"\"\n",
    "    #distance features\n",
    "    dot_m = dot_matrix(image)\n",
    "    dist_m = dist_matrix(dot_m)\n",
    "    dists = dist_array(dist_m)\n",
    "    norm_factor = np.linalg.norm(image[0]-image[3]) # dist(1,17)\n",
    "    dists = dists / norm_factor\n",
    "    #angles features\n",
    "    angles = angle_array(dot_m, dist_m)\n",
    "    #flatten and concat\n",
    "    features_vector = np.around(np.concatenate((dists, angles)),decimals = 2)\n",
    "    return np.append(f[-8:-4],features_vector)\n",
    "\n",
    "def extract_features_forall(images,files):\n",
    "    \"\"\"\n",
    "    input - ndarray of images facial landmarks (for each image a 68 long nparry of points)\n",
    "    output - dataframe of images features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    cols = []\n",
    "    for i in range(len(images)):\n",
    "        if len(images[i]) != 0:\n",
    "            features.append(extract_features(images[i],files[i]))\n",
    "    cols = [\"Person\"] + [\"dist_{1:d}_{0:d}\".format(REF_POINTS[i], REF_POINTS[j]) for i in range(len(REF_POINTS)) for j in range(i)]\n",
    "    for i in range(len(REF_POINTS)):\n",
    "        for j in range(i):\n",
    "            for k in range(j):\n",
    "                cols.append(\"angle_{2:d}_{1:d}_{0:d}\".format(REF_POINTS[i], REF_POINTS[j], REF_POINTS[k]))\n",
    "                cols.append(\"angle_{1:d}_{2:d}_{0:d}\".format(REF_POINTS[i], REF_POINTS[j], REF_POINTS[k]))\n",
    "                # cols.append(\"angle_{2:d}_{0:d}_{1:d}\".format(REF_POINTS[i], REF_POINTS[j], REF_POINTS[k]))\n",
    "    df = pd.DataFrame(features, columns=cols)\n",
    "    return df\n",
    "\n",
    "def prepare_balanced_data(csvPaths, portionCount, testPart = 0.1, m_random_state = 33):\n",
    "    assert testPart<=1\n",
    "    test_threshhold = int(portionCount*(testPart))\n",
    "    data_df_tmp = pd.read_csv(csvPaths[0])\n",
    "    data_df_tmp['emotion'] = [0]*data_df_tmp.shape[0]\n",
    "    data_df_tmp = shuffle(data_df_tmp, random_state=m_random_state)\n",
    "    data_df_test = data_df_tmp[:test_threshhold]\n",
    "    data_df_train = data_df_tmp[test_threshhold:]\n",
    "    for i in range(len(csvPaths)-1):\n",
    "        data_df_tmp = pd.read_csv(csvPaths[i+1])\n",
    "        data_df_tmp['emotion'] = [i+1]*data_df_tmp.shape[0]\n",
    "        data_df_tmp = data_df_tmp[:portionCount]\n",
    "        data_df_tmp = shuffle(data_df_tmp, random_state=m_random_state)\n",
    "        data_df_test = data_df_test.append(data_df_tmp[:test_threshhold])\n",
    "        data_df_train = data_df_train.append(data_df_tmp[test_threshhold:])\n",
    "    data_df_test = shuffle(data_df_test, random_state=m_random_state)\n",
    "    data_df_train = shuffle(data_df_train, random_state=m_random_state)\n",
    "    #delete first img id col\n",
    "    data_df_train = data_df_train.drop(data_df_train.columns[0], axis=1)\n",
    "    data_df_test = data_df_test.drop(data_df_test.columns[0], axis=1)\n",
    "    return data_df_train,data_df_test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1. prepare data - prepare neutral DF\n",
    "NEUTRAL_FOLDER = r\"C:\\Users\\DELL1\\Documents\\studies\\FinalProject\\Datatsets\\datasetAll\\neutral\"\n",
    "#NEUTRAL_FOLDER = r\"C:\\Users\\DELL1\\Documents\\studies\\FinalProject\\Datatsets\\me\\neutral\"\n",
    "neutral_faces_landmarks, neut_files = extract_dlib_facial_points(NEUTRAL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "netural_features = extract_features_forall(neutral_faces_landmarks, neut_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = [\"n%s\"%c for c in netural_features.columns]\n",
    "netural_features.columns = n_cols\n",
    "netural_features.to_csv(r\"C:\\Users\\DELL1\\Documents\\studies\\FinalProject\\Datatsets\\me\\neutral\\neutral_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing disgust...\n",
      "Processing anger...\n"
     ]
    }
   ],
   "source": [
    "for e in EMOTIONS[5:]:\n",
    "    print(\"Processing %s...\"%e)\n",
    "    folder = r\"C:\\Users\\DELL1\\Documents\\studies\\FinalProject\\Datatsets\\datasetAll\\%s\"%e\n",
    "    faces_landmarks, files = extract_dlib_facial_points(folder)\n",
    "    features = extract_features_forall(faces_landmarks, files)\n",
    "    features.to_csv(\"./%s_features.csv\"%e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(247, 8557)\n",
      "(184, 8557)\n",
      "(128, 8557)\n",
      "(201, 8557)\n",
      "(115, 8557)\n",
      "(164, 8557)\n",
      "(147, 8557)\n"
     ]
    }
   ],
   "source": [
    "for e in EMOTIONS:\n",
    "    print(pd.read_csv(\"./%s_features.csv\"%(e)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute deltas...\n",
      "Processing happy...\n",
      "Processing sadness...\n",
      "Processing surprise...\n",
      "Processing fear...\n",
      "Processing disgust...\n",
      "Processing anger...\n"
     ]
    }
   ],
   "source": [
    "print(\"compute deltas...\")\n",
    "neutral_df = pd.read_csv(\"./neutral_features.csv\")\n",
    "for e in EMOTIONS[1:]:\n",
    "    print(\"Processing %s...\"%e)\n",
    "    df = pd.read_csv(\"./%s_features.csv\"%e)\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    cols = df.columns\n",
    "    #print(\"Debug - 1\")\n",
    "    df = pd.merge(df, neutral_df, how = 'left', left_on = 'Person', right_on = 'nPerson')\n",
    "    for c in cols[1:]:\n",
    "        #print(c)\n",
    "        df[c] = df[c]-df[\"n%s\"%c]\n",
    "    df = df.drop(neutral_df.columns, axis=1)\n",
    "    df.to_csv(\"./delta_%s_features.csv\"%e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = []\n",
    "test_df = []\n",
    "csvPaths = [\".//delta_%s_features.csv\"%e for e in EMOTIONS[1:] ]\n",
    "train_df,test_df = prepare_balanced_data(csvPaths, 130, testPart=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process to workable dfs\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "X_train = train_df.iloc[:, 1:-1].as_matrix()    #data\n",
    "Y_train = train_df['emotion'].as_matrix()\n",
    "X_test = test_df.iloc[:, 1:-1].as_matrix()      #data\n",
    "Y_test = test_df['emotion'].as_matrix()            #labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Prediction Tries</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Logistic Regression</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression score with c=0.000100 is: 0.589744\n",
      "LogisticRegression score with c=0.001000 is: 0.679487\n",
      "LogisticRegression score with c=0.010000 is: 0.846154\n",
      "LogisticRegression score with c=0.100000 is: 0.858974\n",
      "LogisticRegression score with c=1.000000 is: 0.846154\n",
      "LogisticRegression score with c=10.000000 is: 0.807692\n",
      "LogisticRegression score with c=100.000000 is: 0.794872\n"
     ]
    }
   ],
   "source": [
    "Cs = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for c in Cs:\n",
    "    m_log_reg = log_reg_classifier(X_train,Y_train,c)\n",
    "    s = m_log_reg.score(X_test,Y_test)\n",
    "    print(\"LogisticRegression score with c={0:f} is: {1:f}\".format(c,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_log_reg = log_reg_classifier(X_train,Y_train,0.1)\n",
    "s = m_log_reg.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error - happy (predicted) - fear (true)\n",
      "Error - fear (predicted) - happy (true)\n",
      "Error - surprise (predicted) - fear (true)\n",
      "Error - fear (predicted) - disgust (true)\n",
      "Error - anger (predicted) - sadness (true)\n",
      "Error - disgust (predicted) - anger (true)\n",
      "Error - disgust (predicted) - surprise (true)\n",
      "Error - disgust (predicted) - anger (true)\n",
      "Error - sadness (predicted) - anger (true)\n",
      "Error - fear (predicted) - sadness (true)\n",
      "Error - anger (predicted) - sadness (true)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_test)):\n",
    "    p = m_log_reg.predict([X_test[i]])\n",
    "    if(p != Y_test[i]):\n",
    "        print(\"Error - %s (predicted) - %s (true)\"%(EMOTIONS[p[0]+1],EMOTIONS[Y_test[i]+1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>MLPClassifier</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1         0.31622777  1.          3.16227766 10.        ]\n",
      "Scores:\n",
      "alpha 0.1 - 0.871795\n",
      "alpha 0.31622776601683794 - 0.858974\n",
      "alpha 1.0 - 0.871795\n",
      "alpha 3.1622776601683795 - 0.858974\n",
      "alpha 10.0 - 0.858974\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-1, 1, 5)\n",
    "names = []\n",
    "classifiers = []\n",
    "for i in alphas:\n",
    "    names.append('alpha ' + str(i))\n",
    "    classifiers.append(MLPClassifier(alpha=i, random_state=11))\n",
    "# iterate over classifiers\n",
    "print(alphas)\n",
    "print(\"Scores:\")\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train, Y_train)\n",
    "    score = clf.score(X_test, Y_test)\n",
    "    print(\"{0:s} - {1:f}\".format(name, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlpclassifier score is  0.8717948717948718\n"
     ]
    }
   ],
   "source": [
    "alpha002 = 1\n",
    "mlpclassifier = MLPClassifier(alpha=alpha002, random_state=11)\n",
    "mlpclassifier.fit(X_train, Y_train)\n",
    "score = mlpclassifier.score(X_test, Y_test)\n",
    "print(\"mlpclassifier score is \",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error - disgust (predicted) - happy (true)\n",
      "Error - happy (predicted) - fear (true)\n",
      "Error - fear (predicted) - happy (true)\n",
      "Error - fear (predicted) - disgust (true)\n",
      "Error - disgust (predicted) - anger (true)\n",
      "Error - disgust (predicted) - surprise (true)\n",
      "Error - disgust (predicted) - anger (true)\n",
      "Error - sadness (predicted) - anger (true)\n",
      "Error - fear (predicted) - sadness (true)\n",
      "Error - anger (predicted) - sadness (true)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_test)):\n",
    "    p = mlpclassifier.predict([X_test[i]])\n",
    "    if(p != Y_test[i]):\n",
    "        print(\"Error - %s (predicted) - %s (true)\"%(EMOTIONS[p[0]+1],EMOTIONS[Y_test[i]+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Sanity Check </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "faces_landmarks, files = extract_dlib_facial_points(r\"C:\\Users\\DELL1\\Documents\\studies\\FinalProject\\Datatsets\\me\\more\")\n",
    "features = extract_features_forall(faces_landmarks, files)\n",
    "features['Person'] = ['me']*5\n",
    "features.to_csv(r\"C:\\Users\\DELL1\\Documents\\studies\\FinalProject\\Datatsets\\me\\more\\features_me.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute deltas...\n"
     ]
    }
   ],
   "source": [
    "print(\"compute deltas...\")\n",
    "neutral_df = pd.read_csv(r\"C:\\Users\\DELL1\\Documents\\studies\\FinalProject\\Datatsets\\me\\neutral\\neutral_features.csv\")\n",
    "df = pd.read_csv(r\"C:\\Users\\DELL1\\Documents\\studies\\FinalProject\\Datatsets\\me\\more\\features_me.csv\")\n",
    "neutral_df = neutral_df.drop(neutral_df.columns[0], axis=1)\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "cols = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, neutral_df, how = 'left', left_on = 'Person', right_on = 'nPerson')\n",
    "for c in cols[1:]:\n",
    "     df[c] = df[c]-df[\"n%s\"%c]\n",
    "df = df.drop(neutral_df.columns, axis=1)\n",
    "df.to_csv(r\"C:\\Users\\DELL1\\Documents\\studies\\FinalProject\\Datatsets\\me\\delta_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sanity_df = pd.read_csv(r\"C:\\Users\\DELL1\\Documents\\studies\\FinalProject\\Datatsets\\me\\delta_features.csv\")\n",
    "sanity_df = sanity_df.drop(sanity_df.columns[0], axis=1)\n",
    "X_sanity = sanity_df.iloc[:, 1:].as_matrix()      #data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = m_log_reg.predict(X_sanity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0 -2  1  0]\n"
     ]
    }
   ],
   "source": [
    "a = [ 1,2,3,4,5]\n",
    "print(ps-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'modelLF.dat'\n",
    "pickle.dump(m_log_reg, open(filename, 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyQt5 import QtCore, QtGui, QtWidgets, QtMultimedia\n",
    "from PyQt5.QtMultimedia import *\n",
    "from PyQt5.QtCore import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAYLISTS_PATH = r\"C:\\Users\\DELL1\\Documents\\GitHub\\ExpressionRecognition\\Application\\Playlists\"\n",
    "class MoodPlayLists(QtMultimedia.QMediaPlayer):\n",
    "    def __init__(self, parent=None):\n",
    "        super().__init__(parent)\n",
    "        moods = glob.glob(PLAYLISTS_PATH +\"//*\")\n",
    "        self.playlists = []\n",
    "        for m in moods:\n",
    "            songs = glob.glob(m + \"//*\")\n",
    "            for s in songs:\n",
    "                playlist = QMediaPlaylist()\n",
    "                url = QUrl.fromLocalFile(s)\n",
    "                playlist.addMedia(QMediaContent(url))\n",
    "            playlist.setPlaybackMode(QMediaPlaylist.Loop)\n",
    "            self.playlists.append(playlist)\n",
    "    \n",
    "    def change_playlist(self, mood=0):\n",
    "        self.setPlaylist(self.playlists[mood])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MoodPlayLists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'sip.wrappertype' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-82bbfeec0b1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplaylist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQMediaPlaylist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplaylist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetPlaybackMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQMediaPlaylist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQMediaPlaylist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'sip.wrappertype' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "playlist = QMediaPlaylist()\n",
    "playlist.setPlaybackMode(QMediaPlaylist.Loop)\n",
    "type(QMediaPlaylist.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
